= CDB Vector Data in GeoPackage Interoperability Experiment

== CAE Report

Author: Bernard Leclerc and Jonathan Rodriguez

=== Focus of the Experiment

We focused on Experiment #2 described on the OGC portal at this address:

  https://portal.opengeospatial.org/?m=projects&a=view&project_id=592

In short, we focused on the part of the experiment where each Shapefile is replaced by one GeoPackage file.

We performed a series of tests to measure the impact of Shapefile-to-GeoPackage conversion on file size, file count, network transfer, and decode performance. These tests are detailed in the following sections.

In the final section, we conclude with some observations and recommendations for futher development of the standard.

=== Comparing File Formats

The first step of our experiment was to compare other file formats that would be appropriate to store CDB Vector Data. These formats include GeoJSON and GML in addition to Shapefiles and GeoPackage. The table below lists the observed file sizes.


.Converted File Sizes
[options="header"]
|=========================
| |Shapefile | GeoPackage | GeoJSON | GML
| Small input   >|  0.5 KB >|  112 KB >|  0.5 KB >|  3.6 KB
| Medium input  >|   32 KB >|  152 KB >|   48 KB >|  106 KB
| Large input   >|  336 KB >|  520 KB >|  824 KB >|  928 KB
| Largest input >| 2686 KB >| 3084 KB >| 4410 KB >| 9008 KB
|=========================

Notes and observations:

* GeoPackages are very space-inefficient at encoding small numbers of features. An amount of data that requires 0.5 KB in Shapefile or GeoJSON encoding was observed to require 112 KB as outputted by `ogr2ogr`. However, not all database objects generated by `ogr2ogr` appear to be strictly required by the standard, and we were able to construct a GeoPackage that should be minimally standard-compliant in as little as 36 KB (by not including optional tables, indexes, triggers, or sequences).
* The actual space required on disk will increase these numbers to varying degrees; the minimum disk space required for a Shapefile in this test was observed to be 12 KB (3 files times 4 KB allocation unit size).
* GeoJSON and GML are less space-efficient than binary alternatives at larger file sizes. GML is particularly large, being in some cases more than twice the size of GeoJSON and thrice the size of the Shapefile. However, GeoJSON and GML remain interesting from the standpoint of interoperability.

Notably, we also observed that every new table added to an sqlite database increases the size by a minimum of 4 KB, which is presumably an internal allocation unit intended to support real-time addition of data rows.

We also did a simple read-performance test on each of these files. For this test, we measured the time taken by GDAL to open the file, iterate through all features, then close the file. All measurements represent the mean of 8 runs.

.Initial Read Performace Test
[options="header"]
|=========================
| | Shapefile | GeoPackage | GeoJSON | GML
| Small input   >|   2.3 ms >|  4.6 ms >|   0.7 ms >|   2.3 ms
| Medium input  >|   5.5 ms >|  4.6 ms >|   4.3 ms >|   4.7 ms
| Large input   >|  14.3 ms >|  6.7 ms >|  53.0 ms >|  18.6 ms
| Largest input >| 278.5 ms >| 40.4 ms >| 347.0 ms >| 183.3 ms
|=========================

Notes and observations:

* GeoPackage read performance scales extremely well at these file sizes. However, there is a fixed overhead that is rather larger than the other file formats.

* GML read performance scales favorably to Shapefiles, with a fixed overhead comparable to Shapefiles (at least in GDAL).

* GeoJSON has a very low fixed overhead, but scales surprisingly badly. We strongly suspect that this performance problem is due to GDAL's use of the `libjson` library. For future performance tests with this format, we strongly recommend using RapidJSON ( http://rapidjson.org/ ) or other comparably fast JSON parser. At least one benchmark ( https://github.com/mloskot/json_benchmark ) reports libjson as being 30 times slower than RapidJSON for GeoJSON data, and another benchmark reports that libjson does not correctly handle UTF-8 data ( https://github.com/miloyip/nativejson-benchmark ), which could be an interoperability issue. With a faster JSON parser, we expect performance to be similar to GML or even faster due to smaller file size.


=== Modifications to the GDAL/OGR Library

In preparation for more comprehensive tests, we then made a few minor modifications to the GDAL library to ensure that the Shapefile-to-GeoPackage conversion was sufficiently lossless for our purposes.

We made the following modifications:

* Stopping the library from optimizing away the M dimension in the case of 2D measured geometry. (This optimization saves some space where all M values are `nodata`, but it changes the declared type of the geometry.)

* Mapping the `Logical` DBF field type to the OGR field type `OFTInteger` subtype `OFSTBoolean`. The DBF logical field type was previously handled as a string.
* Mapping the DBF logical values "T" and "F" to "1" and "0", respectively.
* Allowing the DBF reader to correctly read dates with the format `YYYY/MM/DD`.

=== Converting a Full CDB

We did not use the sample CDB provided by the participants because our goal was to compare the performance of our internal applications when running with a CDB produced by CAE before and after the replacement of Shapefiles with GeoPackage files.

However, we believe that our findings may apply equally well to other databases.

We built a customized script that invoked the `ogr2ogr` executable to convert all vector files in two CDBs. Class DBF files and junction DBF files were converted to standalone GeoPackage files.

We found that the conversion to GeoPackages can substantially increase the amount of disk space required for vector data.

.Disk Space Required
[options="header"]
|=========================
| | ESRI Shapefiles | ESRI Shapefiles (disk) | GeoPackages
| CDB 1 >|  10.1 GB >|  10.4 GB >|  16.4 GB
| CDB 2 >|  12.4 GB >|  20.6 GB >| 119.1 GB
|=========================

Notes and observations:

* Some of our CDBs have a very large number of very small vector files. This leads to an increase in disk space usage: CDB 2 in particular requires 20.6 GB to store 12.4 GB of Shapefiles (assuming a 4 KB allocation unit).

* We have not done a complete measurement of file size distributions, but in the case of CDB 1, we do know that over 40% percentage of the shapefiles consist of a single .dbf. The median ESRI Shapefile size (sum of .shp/.shx/.dbf/.dbt) is about 3 KB, lower quartile under 1 KB, upper quartile 38 KB. Only 20% of the shapefiles are larger than 112 KB.

* CDB 2, which we believe appproaches a worst-case scenario in terms of disk usage increase, has a nearly 6 times increase in on-disk vector data-storage requirements (from 20.6 GB to 119.1 GB). This constitutes a non-negligible risk.


=== Network Test

Our application of CDB involves networked client/server systems, so a key performance factor is the time required to request and transfer files over a network. For this test, we measured the time taken for a client to request and transfer a selection of vector data files from a networked server. Files were loaded "cold": the CDB data volume was freshly mounted immediately before each test to ensure that the OS file cache was clear. This test loaded files from CDB 2.

.Network Test
[options="header"]
|=========================
| | ESRI Shapefiles | GeoPackages
| Request/Open/Transfer Time >| 5132 ms >| 3475 ms
| Files Transferred >| 669 files >| 223 files
| Data Transferred >| 107 MB >| 161 MB
|=========================

Notes and observations:

* This test does not load class- or extended-attribute files.

* The file-count reduction ratio is 3:1, not 4:1. We do not store 0-byte files if we can avoid it.

* The amount of data transferred is larger for GeoPackages than for Shapefiles, but the number of files requested is substantially smaller. The largest performance factor in this test seems to be the reduction in the number of files requested, not the I/O volume.

* The data transfer increase was only about 1.5x, compared with a 9.6x increase (12.4 GB to 119.1 GB) in total vector data for this CDB. This test should therefore not be taken an indication of worst-case performance, and suggests that the density of geographic features could vary considerably from location to location. Determination of an accurate worst-case performance profile would require more extensive experiments.

=== Real-Time CDB Client Device

The final test is to benchmark the loading time for a certain geographical region in a real time system. We measured the decode time, number of files and data transfer volume. The real time system is the client device consuming OGC CDB data over the network. This test loaded files from both CDB 1 and CDB 2.

.Real-Time CDB Client Test
[options="header"]
|=========================
| | ESRI Shapefiles | GeoPackages | difference (+/-)
| Decode-only Time >| 7.37 s >| 5.65 s (GDAL [*]) +
10.81 s (internal) >| +9.09 s
| Files decoded >| 5680 files >| 2838 files >| 50% fewer files
| Data transferred >| 479 MB >| 906 MB >| 89.1% more bytes
|=========================

[*] Our observed GeoPackge decode time is split into GDAL-related processing and internal format conversion (which is unoptimized). Although our total GeoPackage decode time was measured at 9.09 seconds slower than Shapefile decode time, the GDAL-only portion of the decode time 1.72 seconds faster. If we were to write a well-optimized GeoPackage decoder that decoded directly from sqlite into our internal representation, it would be reasonable to expect a small performance win.

Notes and observations:

* This test loads class- and extended-attribute files where present.

* The file-count reduction ratio is 2:1, not 4:1. We do not store 0-byte files if we can avoid it.

* We did observe a slight overall slowdown in the system, but the total slowdown was less than the 9.09 seconds observed in the decode process. This suggests that the performance gained by halving the file count was greater than the performance lost by doubling the I/O bandwidth requirements.

=== Conclusions and Recommendations

Although previous experiments with 1-to-1 Shapefile-to-GeoPackage conversion have been very positive, our experiences indicate mixed results. On one hand, we observe substantial benefit in reducing the number of files stored and loaded, and we also observe the possibility of a comparable (or faster) decode time; but on the other hand, we see potentially-problematic increases in the amount of storage space required.

The negative effects are amplified for CDBs with large numbers of small Shapefiles. The GeoPackage format (especially as outputted by GDAL) is very space-inefficient when it comes to encoding tiles with small quantities of data. (Notably, even if we moved to a 1-tile-per-table instead of a 1-tile-per-file approach, the underlying sqlite format seems to allocate a minimum of 4 KB per table regardless of how little data is in it.)

==== Suggestion 1: Storage versus Transport Format

We have essentially been using GeoPackage as a data _transfer_ format in this experiement, when its design seems to be much better used as a data _storage_ format. The GeoPackage format encodes indexes, triggers, sequences, and metadata as well as feature data, and it also supports real-time addition, removal, and update of records. In this experiment, we make use of none of these features.

We suggest considering GeoJSON as a candidate format for the 1-to-1 Shapefile conversion case (Option 1a). We may, with GeoJSON, obtain the same reduction in file count while simultaneously obtaining better performance and storage characteristics for CDBs with large numbers of small vector files. To obtain scalable performance characteristics with GeoJSON, we will want to base implementations on RapidJSON or similar parser (rather than libjson, which is currently used by GDAL).

It is worth noting here also that GeoJSON supports other variants of the Option 1 experiment, e.g., attribute flattening (Options 1c/1d).

==== Suggestion 2: GeoPackage as an Incremental Data Store Version

In this experiement, we have explored the idea of placing GeoPackages _inside_ a CDB. We suggest that this may not be the best approach for maximizing compatibility. GeoPackage, like CDB, functions conceptually as an independent data store; we would like to raise the possibility of using GeoPackage as an incremental data store _version_, which would essentially allow a GeoPackage to replace a CDB version at its root (at least to the extent that all data inside the CDB can be converted losslessly into GeoPackage data). The idea is to be able to add a GeoPackage as an incremental version without modifying the underlying CDB, or vice versa. What we would have to do in this case is define a bidirectional equivalency between a CDB directory path and a GeoPackage/sqlite index--this would allow us clearly-defined semantics for mixing and matching GeoPackage and CDB data stores, with minimal impact on existing standards and implementations.

==== Suggestion 3: The CDB Directory Hierarchy as a Key-Value Store

We would like to raise a particular opportunity for future-proofing the standard. Conceptually, the CDB directory hierarchy functions as an index: any given directory path is essentially a key, and the value accessed by the key is a file. If we introduce a level of abstraction that allows us to discuss the CDB as a type of key-value store, then we open up a range of new possibilities in terms of physical implementation. For example, there are any number of database engines that are able to function as fast key-value stores, from lightweight mobile solutions like sqlite to highly-distributed cloud-capable NoSQL solutions like MongoDB. This would conceptually simplify the idea of a geographic database, allowing implementors more freedom to choose the storage technology that best suits them while simultaneously providing a natural path toward remote/Internet query of CDBs.
