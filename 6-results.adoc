[[ResultsClause]]
== IE Experiment Results

This section of the Engineering Report provides details of the results of the experiments performed by each of the IE participants.

=== Aechelon Technology IE Report

==== Use case and experiment focus

The following is a description of a key use case in the Aechelon content processing workflow to use in an image generator.

* A CDB data store is used as the source for content to feed a publishing process into the Aechelon Image Generator (IG) runtime format. While reducing CDB storage requirements is a consideration, the primary concern in this workflow is with read access speed. Even so, the time taken by the 'feature scan' step of the publishing process, where the CDB source vector files are scanned to identify the features to import, is 2 orders of magnitude smaller than the rest of the pipeline. However, any improvements in speed and/or storage requirements have a positive impact on the efficiency of the Achelon publication workflow.
* Note: After the feature scan step in the publishing process, all references to all features are in various python data structures, which are then given to the first of multiple processing steps to begin the data transformations. For example, point features with models will have their OpenFlight models converted to an intermediate Aechelon-specific model format, while their instance geographical data are saved in an Aechelon-specific lookup table format.

The publishing software is in python, with invocations of C++ EXEs for performance-critical processing. The feature scan step is entirely in Python, version 3.5. Please note that for the image generation workflow, only metadata fields that affect the appearance of features are considered and remainder of the CDB content is ignored, such as tactical data, or the entire geopolitical dataset. The hydrography network dataset is also ignored since the RMTexture dataset is used to identify areas of water. 

For the purposes of this experiment, five feature types were considered and processed: Cultural, Lights, Powerlines, Railroads, and Trees. All of the performance information provided in the tables below is related to these five feature types.

==== Aechelon Experiment Methodology

The following is a concise description of the methodology used for execution of the Aechelon committed tasks in this Interoperability Experiment.

* Following generation of the GeoPackage files for each option, changes were made as needed in the publishing scripts to be able to read and publish the GeoPackage structured data.
* Each feature type was tested by spot-checking in the image generator using a small reference database with representative data for each of the five feature types.
* The next step was to convert the following three of the four CDB data stores made available for the interoperability experiments. However, the entire publication end-to-end workflow for image generation was not performed due to the considerable time it would take for each run. CDB data processed for the Aechelon experiments were:
  ** Yemen (4 geocells), from Presagis.
  ** Downtown Los Angeles (1 geocell), from VATC.
  ** Greater Los Angeles (4 geocells), from Cognitics.
* To generate data for Experiment 2, option 1A:
  ** Ran Option1.py from the Cognitics conversion scripts in the master branch (https://github.com/Cognitics/GeoCDB/tree/master).
  ** Deleted existing .shp, .shx, sidecar .dbf, .dbt & .prj files (i.e. kept .dbf files holding class/extended data.)
* To generate data for Experiment 2, option 1C:
  ** Ran Option1.py from the Cognitics conversion scripts in the Presagis branch (https://github.com/Cognitics/GeoCDB/tree/Presagis).
  ** Deleted the existing .shp, .shx, .dbf, .dbt & .prj files.
* To generate data for Experiment 2, option 1D:
  ** Ran Option1d.py from the Cognitics conversion scripts in the master branch (after update of March 17, 2019, and some local edits to protect against 'None' during conversion of the LA databases.)
  ** Deleted the existing .shp, .shx, .dbf, .dbt & .prj files.
* To generate data for Experiment 3:
  ** Ran Option3.py from the Cognitics conversion scripts in the master branch (after update of March 17, 2019, and some local edits to uncomment writing the class metadata to the instance tables and to protect against 'None' during conversion of the LA databases.)
  ** Deleted the 100_GSFeature, 101_GTFeature, 202_RailroadNetwork and 203_PowerlineNetwork folders from each geocell.
* To generate data for Experiment 4:
  ** Ran Option4.py from the Cognitics conversion scripts in the master repository (after update of March 17, 2019, and some local edits to uncomment writing the class metadata to the instance tables and to protect against 'None' during conversion of the LA databases.)
  ** Deleted the 100_GSFeature, 101_GTFeature, 202_RailroadNetwork and 203_PowerlineNetwork folders from each geocell.
* Then, for each option, disabled the publishing process beyond the 'feature scan' step and captured the following metrics for the three CDB data stores.

==== Metrics

The following three tables provide basic performance metrics for the three CDB data stores processed in this experiment. Providing performance metrics is one of the tasks identified in Experiment 1 of the GeoPackage in CDB IE.

In the tables below, "Baseline" refers to metrics based on the source Shapefiles. Option 1A, Option 1C, and Option 1D refer to the sub-options for Experiment 2 (one to one transformation of Shapefiles into GeoPackages).

.Yemen (4 geocells)
[width="90%",options="header"]
|===
|           |           |          |Baseline  |Option 1A |Option 1C |Option 1D |Exper. 3 |Exper. 4     
|Dataset(s) |Feat count |PVF count |     time |     time |     time |     time |    time |    time
|tree       |     64091 |     440  |        8 |        7 |        7 |       16 |       6 |       2
|light      |        60 |      13  |       <1 |       <1 |       <1 |       <1 |       1 |       1
|cultural   |     16502 |     409  |       12 |        9 |        5 |        7 |       5 |       4
|powerline  |       975 |      20  |       <1 |       <1 |       <1 |       <1 |       1 |       1
|railroad   |         0 |       0  |        0 |        0 |        0 |        0 |       0 |       0
|total time |  | |                         21 |       17 |       13 |       24 |      14 |       8
|file count |  | |                       8224 |     2056 |     1023 |     1023 |      10 |      10           
|size (MB)  |  | |                        34.2|     152.5|     161.9|     165.9|     57.6|     38.1 
|===

                                                                                               
.Downtown Los Angeles (1 geocell)
[width="90%",options="header"]
|===
|           |           |          |Baseline  |Option 1A |Option 1C |Option 1D |Exper. 3 |Exper. 4     

|Dataset(s) |Feat count |PVF count |     time |     time |     time |     time |    time |    time
|tree       |        2  |        1 |       <1 |       <1 |       <1 |       <1 |      <1 |      <1
|light      |        0  |        0 |        0 |        0 |        0 |        0 |       0 |       0
|cultural   |  1730622  |     1948 |     9:01 |     7:13 |     3:06 |     3:34 |    3:26 |    3:41
|powerline  |     1208  |       56 |        2 |        1 |        1 |        1 |      <1 |       1
|railroad   |     1386  |        4 |        1 |       <1 |       <1 |       <1 |      <1 |       1
|total time |   ||                      9:04 |     7:15 |     3:08 |     3:36 |    3:27 |    3:44          
|file count |   ||                     12540 |     4180 |     2090 |     2090 |       4 |       4       
|size (MB)  |   ||                     2185.7|    2309.2|     958.5|    1021.5|    791.5|    798.0
|===

.Greater Los Angeles (4 geocells)
[width="90%",options="header"]
|===
|           |           |          |Baseline  |Option 1A |Option 1C |Option 1D |Exper. 3 |Exper. 4     

|Dataset(s) |Feat count |PVF count |     time |     time |     time |     time |    time |    time
|tree       |        5  |        2 |       <1 |        1 |       <1 |       <1 |       1 |      <1
|light      |        0  |        0 |        0 |        0 |        0 |        0 |       1 |      <1
|cultural   |  3138841  |     6013 |    15:02 |    12:02 |     6:14 |     7:25 |    6:57 |    7:17
|powerline  |     3932  |      160 |        1 |        1 |        1 |        1 |       1 |       1
|railroad   |     9367  |       87 |        1 |        1 |        1 |        1 |       1 |      <1
|total time |      ||                   15:04 |    12:05 |     6:16 |     7:27 |    7:01 |    7:19
|file count |      ||                   38961 |    12986 |     6493 |     6493 |      14 |      14         
|size (MB)  |      ||                   3738.2|    4275.9|    1958.6|    2067.0|   1335.7|   1339.3
|===

==== Legend

* Feat count: feature count of valid features found of the given type
* PVF count: primary vector file count, after validation, for the given type (i.e. only counting .shp files for Experiment 1 or .gpkg files for Experiment 2.)
* Time: in minute:second notation when over 1 minute, else in seconds
* The cultural feature data set is from both 100_GSFeatures (S001_T001 & S002_T001) and 101_GTFeatures (S001_T001)
* File count: total number of files from 100_GSFeatures, 101_GTFeatures, 202_RailroadNetwork & 203_PowerLineNetwork
* Size: storage, in MB, used by all the files from 100_GSFeatures, 101_GTFeatures, 202_RailroadNetwork & 203_PowerLineNetwork

==== Notes and observations
* All source CDB files were on a local RAID drive so network traffic did not contribute to the timings.
* In the Greater Los Angeles database, there somehow were more features of some types coming from geopackage files compared to Shapefiles (3140180 instead of 3138841 cultural features, and 4012 instead of 3932 powerline features), but there were also over 1000 warnings from OGR during conversion and while reading of the type "Warning 1: Unable to parse srs_id '100000' well-known text ''." After the 1000th such warning, also got "More than 1000 errors or warnings have been reported. No more will be reported from now."  Perhaps the conversion from .shp to .gpkg with ogr2ogr.exe generated these excess invalid files. These warnings appeared in the Downtown LA database as well, but the feature counts matched after conversion. Checking any further downstream for discrepencies in the processing pipeline was not performed.
* For the powerline network dataset, statistics include both the tower point features and the wire lineal features.
* There's a slight increase in the file size in the Los Angeles databases when comparing the results of Experiment 3 and Experiment 4. However, there is a significant decrease in the size of the Yemen database. From a quick inspection of the data, this seems to correlate with the fact that almost all the cultural features in Los Angeles come from 100_GSFeatures which require unique records per instance, whereas for Yemen the majority of cultural features come from 101_GTFeatures.
* Experiment 3 has slightly better timings for large-count datasets than Experiment 4 in our use case since we scan each LOD in order, so having LODs in separate layers in the option 3 GeoPackage performs better.

=== CAE Results for Experiment 2

==== Focus of the Experiment

CAE focused on Experiment #2 as described in Section 5 - Details of the Experiments Performed. Specifically, CAE focused on the part of the experiment where each Shapefile is replaced by one GeoPackage file.

CAE performed a series of tests to measure the impact of Shapefile-to-GeoPackage conversion on file size, file count, network transfer, and decode performance. These tests are detailed in the following sections.

CAE conclusion with some observations and recommendations for futher development of the standard can be found in Section 7 Recommendations.

==== Comparing File Formats

The first step of our experiment was to compare other file formats that would be appropriate to store CDB Vector Data. These formats include GeoJSON and GML in addition to Shapefiles and GeoPackage. The table below lists the observed file sizes.

.Converted File Sizes
[options="header"]
|=========================
| |Shapefile | GeoPackage | GeoJSON | GML
| Small input   >|  0.5 KB >|  112 KB >|  0.5 KB >|  3.6 KB
| Medium input  >|   32 KB >|  152 KB >|   48 KB >|  106 KB
| Large input   >|  336 KB >|  520 KB >|  824 KB >|  928 KB
| Largest input >| 2686 KB >| 3084 KB >| 4410 KB >| 9008 KB
|=========================

Notes and observations:

* GeoPackages are very space-inefficient at encoding small numbers of features. An amount of data that requires 0.5 KB in Shapefile or GeoJSON encoding was observed to require 112 KB as output by `ogr2ogr`. However, not all database objects generated by `ogr2ogr` appear to be strictly required by the standard, and we were able to construct a GeoPackage that should be minimally standard-compliant in as little as 36 KB (by not including optional tables, indexes, triggers, or sequences).
* The actual space required on disk will increase these numbers to varying degrees; the minimum disk space required for a Shapefile in this test was observed to be 12 KB (3 files times 4 KB allocation unit size).
* GeoJSON and GML are less space-efficient than binary alternatives at larger file sizes. GML is particularly large, being in some cases more than twice the size of GeoJSON and three times the size of the Shapefile. However, GeoJSON and GML remain interesting from the standpoint of interoperability.

Notably, we also observed that every new table added to an sqlite database increases the size by a minimum of 4 KB, which is presumably an internal allocation unit intended to support real-time addition of data rows.

We also did a simple read-performance test on each of these files. For this test, we measured the time taken by GDAL to open the file, iterate through all features, then close the file. All measurements represent the mean of 8 runs.

.Initial Read Performace Test
[options="header"]
|=========================
| | Shapefile | GeoPackage | GeoJSON | GML
| Small input   >|   2.3 ms >|  4.6 ms >|   0.7 ms >|   2.3 ms
| Medium input  >|   5.5 ms >|  4.6 ms >|   4.3 ms >|   4.7 ms
| Large input   >|  14.3 ms >|  6.7 ms >|  53.0 ms >|  18.6 ms
| Largest input >| 278.5 ms >| 40.4 ms >| 347.0 ms >| 183.3 ms
|=========================

Notes and observations:

* GeoPackage read performance scales extremely well at these file sizes. However, there is a fixed overhead that is rather larger than the other file formats.

* GML read performance scales favorably to Shapefiles, with a fixed overhead comparable to Shapefiles (at least in GDAL).

* GeoJSON has a very low fixed overhead, but scales surprisingly badly. We strongly suspect that this performance problem is due to GDAL's use of the `libjson` library. For future performance tests with this format, we strongly recommend using RapidJSON ( http://rapidjson.org/ ) or other comparably fast JSON parser. At least one benchmark ( https://github.com/mloskot/json_benchmark ) reports libjson as being 30 times slower than RapidJSON for GeoJSON data, and another benchmark reports that libjson does not correctly handle UTF-8 data ( https://github.com/miloyip/nativejson-benchmark ), which could be an interoperability issue. With a faster JSON parser, we expect performance to be similar to GML or even faster due to smaller file size.


==== Modifications to the GDAL/OGR Library

In preparation for more comprehensive tests, CAE then made a few minor modifications to the GDAL library to ensure that the Shapefile-to-GeoPackage conversion was sufficiently lossless for our purposes.

CAE made the following modifications:

* Stopping the library from optimizing away the M dimension in the case of 2D measured geometry. (This optimization saves some space where all M values are `nodata`, but it changes the declared type of the geometry.)
* Mapping the `Logical` DBF field type to the OGR field type `OFTInteger` subtype `OFSTBoolean`. The DBF logical field type was previously handled as a string.
* Mapping the DBF logical values "T" and "F" to "1" and "0", respectively.
* Allowing the DBF reader to correctly read dates with the format `YYYY/MM/DD`.

==== Converting a Full CDB

CAE did not use the sample CDB provided by the participants because their goal was to compare the performance of our internal applications when running with a CDB produced by CAE before and after the replacement of Shapefiles with GeoPackage files.

However, CAE believe that their findings may apply equally well to other databases.

CAE built a customized script that invoked the `ogr2ogr` executable to convert all vector files in two CDBs. Class DBF files and junction DBF files were converted to standalone GeoPackage files.

CAE found that the conversion to GeoPackages can substantially increase the amount of disk space required for vector data.

.Disk Space Required
[options="header"]
|=========================
| | ESRI Shapefiles | ESRI Shapefiles (disk) | GeoPackages
| CDB 1 >|  10.1 GB >|  10.4 GB >|  16.4 GB
| CDB 2 >|  12.4 GB >|  20.6 GB >| 119.1 GB
|=========================

Notes and observations:

* Some of our CDBs have a very large number of very small vector files. This leads to an increase in disk space usage: CDB 2 in particular requires 20.6 GB to store 12.4 GB of Shapefiles (assuming a 4 KB allocation unit).
* We have not done a complete measurement of file size distributions, but in the case of CDB 1, we do know that over 40% percentage of the Shapefiles consist of a single .dbf. The median ESRI Shapefile size (sum of .shp/.shx/.dbf/.dbt) is about 3 KB, lower quartile under 1 KB, upper quartile 38 KB. Only 20% of the Shapefiles are larger than 112 KB.

* CDB 2, which we believe appproaches a worst-case scenario in terms of disk usage increase, has a nearly 6 times increase in on-disk vector data-storage requirements (from 20.6 GB to 119.1 GB). This constitutes a non-negligible risk.

==== Network Test

Our application of CDB involves networked client/server systems, so a key performance factor is the time required to request and transfer files over a network. For this test, we measured the time taken for a client to request and transfer a selection of vector data files from a networked server. Files were loaded "cold": the CDB data volume was freshly mounted immediately before each test to ensure that the OS file cache was clear. This test loaded files from CDB 2.

.Network Test
[options="header"]
|=========================
| | ESRI Shapefiles | GeoPackages
| Request/Open/Transfer Time >| 5132 ms >| 3475 ms
| Files Transferred >| 669 files >| 223 files
| Data Transferred >| 107 MB >| 161 MB
|=========================

Notes and observations:

* This test does not load class- or extended-attribute files.

* The file-count reduction ratio is 3:1, not 4:1. We do not store 0-byte files if we can avoid it.

* The amount of data transferred is larger for GeoPackages than for Shapefiles, but the number of files requested is substantially smaller. The largest performance factor in this test seems to be the reduction in the number of files requested, not the I/O volume.

* The data transfer increase was only about 1.5x, compared with a 9.6x increase (12.4 GB to 119.1 GB) in total vector data for this CDB. This test should therefore not be taken an indication of worst-case performance, and suggests that the density of geographic features could vary considerably from location to location. Determination of an accurate worst-case performance profile would require more extensive experiments.

==== Real-Time CDB Client Device

The final test is to benchmark the loading time for a certain geographical region in a real time system. We measured the decode time, number of files and data transfer volume. The real time system is the client device consuming OGC CDB data over the network. This test loaded files from both CDB 1 and CDB 2.

.Real-Time CDB Client Test
[options="header"]
|=========================
| | ESRI Shapefiles | GeoPackages | difference (+/-)
| Decode-only Time >| 7.37 s >| 5.65 s (GDAL [*]) +
10.81 s (internal) >| +9.09 s
| Files decoded >| 5680 files >| 2838 files >| 50% fewer files
| Data transferred >| 479 MB >| 906 MB >| 89.1% more bytes
|=========================

[*] Our observed GeoPackge decode time is split into GDAL-related processing and internal format conversion (which is unoptimized). Although our total GeoPackage decode time was measured at 9.09 seconds slower than Shapefile decode time, the GDAL-only portion of the decode time 1.72 seconds faster. If we were to write a well-optimized GeoPackage decoder that decoded directly from sqlite into our internal representation, it would be reasonable to expect a small performance win.

Notes and observations:

* This test loads class- and extended-attribute files where present.
* The file-count reduction ratio is 2:1, not 4:1. We do not store 0-byte files if we can avoid it.
* We did observe a slight overall slowdown in the system, but the total slowdown was less than the 9.09 seconds observed in the decode process. This suggests that the performance gained by halving the file count was greater than the performance lost by doubling the I/O bandwidth requirements.

=== Compusult Results from Experiment 2
Approach: One GeoPackage per LOD per dataset

CDB: CDBYemen_4.0.0

Available Datasets:

- 101_GTFeature
- 100_GSFeature
- 401_Navigation
- 201_RoadNetwork

Number of Shapefiles processed: 358
Number of GeoPackages created: 18
Total byte size of Shapefiles (bytes): 3,569,324
Total byte size of GeoPackages (bytes): 41,715,712
Elapsed time (seconds): 173

=== FlightSafety Experiment Results

==== FlightSafety International's Use Case for CDB
FlightSafety has developed both a CDB generation tool and a CDB Publisher client.  The performance requirements of the CDB Publisher are much greater than CDB generation, so this report will focus on loading and consuming a CDB dataset. The CDB Publisher uses a CDB data store as the source data for building the synthetic environment for FlightSafety's VITAL 1100 image generator system.  These systems are used for pilot training on a variety of flight simulator systems. The Publisher does not do any preprocessing of the CDB dataset; all CDB data that it consumes is discovered and loaded during the publishing.  This approach was chosen due to the world-wide scope of CDB and unknown quantity of content.  The CDB specification's structure makes it easy to find the file(s) containing the data needed for the synthetic environment creation. Based on the flight training system requirements, an appropriate level of detail of vector and model data is discovered and loaded.  The Publisher adapts to the available levels of detail of vector data, and the flight characteristics of the training device. The publishing system is primarily in C++, and the testing was all performed with C++ libraries and code.  The Shapefile API that is tested is a custom FlightSafety library, optimized for faster performance.

==== FlightSafety Experiment Focus
The experiments were focused on just-in-time uses of CDB, similar to how a FlightSafety visual system would use the data. Statistics were collected on the original CDB dataset, and the converted GeoPackage CDB datasets.  These were used to infer the cost of database configuration management and transmission/deployment to a training device. Testing was done on both currently encoded CDB Shapefiles, and on converted GeoPackage encoded files (covering options 1, 3, and 4). Tests focused on the latency of loading files, processing data, and closing files. Tests were done on different conversion options and settings to come up with optimal recommendations

==== FlightSafety Experiment Methodology

This is the methodology used to evaluate, convert and test the CDB datasets using GeoPackage vector encoding.

===== Data Acquisition: 
Three CDB datasets were downloaded (from Cognitics, Presagis, and VATC) and loaded on a system. The datasets were then split into two CDBs, one of which contains all vector data and the other contains everything else.  They were linked together using CDB's versioning mechanism, so that the FlightSafety publisher sees the data as a single dataset. Further:
- Any official or unofficial extension to the CDB was removed for testing purposes.
- Any 0 size vector file was deleted from the CDB with vector data.  These were 0 size shp and shx files for datasets that should only be dbf, and cases of 0 size dbt files when they weren't needed alongside their dbf parent file.

===== Data Evaluation: 
All three CDB datasets were flown using FlightSafety's VITAL 1100 image generator and CDB publisher. During the fly-through, any data artifacts were noted and recorded.

===== Data Conversion: 
The python conversion scripts developed by Cognitics, Inc. were downloaded from GitHub. The scripts were modified to properly flatten class-level attributes into the feature table, and to properly handle DBase floating point and logical field types.  Index tables were also added to aid SQL queries designed to get back data for a specific CDB vector file. Script changes were published to a public GitHub under a FlightSafety account (link). When the scripts were run, they created a new output directory for the CDB vector data.  The Metadata folder was copied from the original vector CDB version, which then links this GeoPackage version to the rest of the CDB data. The three main conversion scripts used implemented GeoPackage encoding options 1, 3, and 4.

===== GeoPackage Testing: 
The initial data collection centered on the number of vector files and how much disk space was consumed.  All full CDB storage devices used a 4kB block size and recorded sizes include the "dead" space due to the minimum block size. The initial tests were testing Shapefiles vs. option 1.  All vector files were located, and timed on the file open and accessing the data within the file.  Total processing time was recorded and compared between the two encodings.  This test accessed the geometry and all the attributes, whether they would have been used by the FlightSafety CDB publisher or not.

The next set of tests involved working with worst case examples and comparing the same file open and access time as before, but for single files.  This highlights performance on the largest vector files.  The average performance times are reported here. 

Further testing was performed to see what the trade offs were between options 1, 3, and 4.  These included loading identical vectors (from a single original Shapefile) from each of three GeoPackage files converted in different ways: 

- GeoPackage Option 1 (Experiment 2 in the IE Activity Plan) was a straight conversion of the Shapefile.  The GeoPackage contains a single data table with flattened class-level attributes, with the same number of records as the original Shapefile
- GeoPackage Option 3 (Experiment 3 in the IE Activity Plan) was a conversion of each CDB dataset's features into a table for each level of detail (LOD) and component selector set, placed into a single GeoPackage (1 per dataset).  It also contained the most tables, and typically had more feature records than option 1 but fewer than option 4.
- GeoPackage Option 4 (Experiment $ in the IE Activity Plan) was a conversion of each CDB dataset's features into a table for each component selector set, placed into a single GeoPackage (1 per dataset).  This method placed all levels of detail into the same table, resulting in a handful of tables, but possibly millions of features per table.

Note: _The results for Experiment 1 (Conversion) are provided in the discussions of Option 1, 3, and 4 (IE Experiments 2, 3, and 4)._

==== FlightSafety Metrics

===== Original Dataset Statistics
Basic statistics were collected on the original CDB datasets used in the Interoperability Experiment.  The CDB storage size and file counts do not include any 0-sized files (they weren't required by the CDB specification) and do not include non-standard extension data.  The last two rows represent the proportion of vector data in the CDB, by the percentage of files and storage used.  The vector datasets used are:
- 100_GSFeature
- 101_GTFeature
- 102_GeoPolitical
- 201_RoadNetwork
- 202_RailroadNetwork
- 203_PowerlineNetwork
- 204_HydrographyNetwork
- 401_Navigation

.Table of Dataset Statistics
[width="90%",options="header"]
|===
|           | Northwest Pacific|  Yemen|Los Angeles
|*Provider*	  |Cognitics	 |Presagis	|VATC
|*CDB Geocell Tiles*|	27|	4|	1
|*CDB Storage Size*|	214 GB	|17.4 GB	|59.6 GB
|*CDB File Count*|	427,536 files	|112,837 files	|62,895 files
|*Vector Storage Size*|	9,152 MB	|53.4 MB	|2,381 MB
|*Vector File Count*|	109,490 files	|4714 files	|13,075 files
|% of CDB storage as vectors	|4.18 %	|0.30 %	|3.90 %
|% of CDB files as vectors	|25.6 %	|4.18 %	|20.8 %
|===

The main takeaway from this table is that vector data does not consume a large amount of storage space, but accounts for a prodigious number of files within a typical CDB.  The main driver of file counts are that Shapefiles are a multi-file format, where three (or four with the .prj projection file) files represent a single Shapefile.  In addition to the multifile format, CDB uses extra class-level and extended-level attributes encoded as extra DBF files.  So anywhere from 3 to 8 files are used to represent a single logical vector file.

===== Specific Vector File Test Data
Some of the testing below involved loading specific point/linear/areal vectors that represent a single Shapefile.  For these tests, examples were found that represent "worst-case" examples of large vector files.  These larger files would take more time to load, and most occurred within higher LODs that would lead to larger tables in options 3 and 4.  The following table records the specific Shapefile data for individual tests.

[width="90%",options="header"]
|===
|  | Northwest Pacific	| Yemen	|Los Angeles
|*Point Vector* |	N46W124_D101_S002_T001_L04_U15_R12	| N12E045_D100_S001_T001_L04_U12_R0	| N34W119_D100_S001_T001_L05_U8_R20
|*Linear Vector* |	N48W123_D201_S002_T003_L01_U0_R0	| N12E045_D201_S002_T003_L00_U0_R0	| N34W119_D201_S002_T003_L04_U1_R15
|*Areal Vector* |	N47W120_D204_S002_T005_L02_U0_R2	| N12E044_D100_S002_T005_L02_U3_R3	| N34W119_D204_S002_T005_L03_U4_R7
|===

==== Shapefile vs. GeoPackage Option 1 (Experiment 2) Testing

===== Option 1 Conversion Statistics
Before the first set of tests, the CDB datasets were converted one-to-one from Shapefiles to GeoPackage, using the option 1 conversion.  Dataset statistics were then collected on the new datasets and compared with the original datasets.

[width="90%",options="header"]
|===
|  | Northwest Pacific	| Yemen	|Los Angeles
|*Shapefile Vector Storage Size* |	9,152 MB	|53.4 MB	|2,381 MB
|*Shapefile Vector File Count* |	109,490 files	|4714 files	|13,075 files
|*GeoPackage 1 Storage Size* |	17,827 MB	|157.9 MB	|938 MB
|*GeoPackage 1 File Count* |	25,083 files	|1,146 files	|2,615 files
|*Relative Size (>1 is larger)* |	1.95	|2.96	|0.39
|*% Fewer Vector Files* |	77 %	|76 %	|80 %
|===

File counts for the GeoPackage CDB were between a 4:1 and 5:1 reduction in vector files.  The size changes varied dramatically, likely due to how efficient the attributes were packed into the original Shapefile's instance and class-level DBF files.  In general, an increase in CDB size is expected using option 1.

===== Option 1 Testing Focus
The testing focused on the latency of loading and processing the vector data files, and traversing all the geometry features and attributes.  This approach was used to simulate a flight simulation client's use of CDB.

===== Test Procedure 1
The first test was to traverse the entire CDB dataset, find all the vector files and collect the time it took to open, process, and close each vector file.  For each dataset, every vector file was located by walking the directory structure, and then the file loading and processing was timed.  This test was run 30 times on the smaller CDB datasets (Yemen and Los Angeles) and 10 times on the larger Northwest Pacific dataset.  The sum of the file load and process steps are recorded below (while ignoring the file search times).

[width="90%",options="header"]
|===
|*All Vector Files*| Northwest Pacific	| Yemen	|Los Angeles
|*Shapefile Timing* |	835 sec	|10.2 sec	|27.5 sec
|*GeoPackage Timing* |	478 sec	|4.2 sec	|25.7 sec
|*GeoPackage Speed Comparison* |	42% faster	|58% faster	|6.7% faster
|*Average Shapefile Storage Size* |	374 kB	|48 kB	|923 kB
|===

This table shows, on average, that using GeoPackages are faster than using Shapefiles.  These results imply that GeoPackage has a better advantage with smaller files. For example, GeoPackage performed best on Yemen with its relatively small Shapefile/vector files.  However, there is less of an advantage with larger vector files. Therefore, further testing using larger files is recommended.

===== Test Procedure 2
The next set of tests focused on some of the largest individual vector files. This test was performed to evaluate some of the worst case examples.  The exact file names are mentioned above in the Specific Vector File Test Data section.  These test datasets were much larger than the average vector file and cover the three basic geometry types: Points, Line Strings and Polygons.  This allowed testing of files that have many attributes compared to coordinates (points), and testing of files with many coordinates compared to the number of attributes (polygons).

- The file size for Shapefiles includes both the instance-level files (.shp, .shx, .dbf) and the class-level attributes (.dbf), but no extended attributes or projection information.  The GeoPackage file size was the single .gpkg file.
- The timing numbers include opening the file and traversing the geometry and every attribute in each record, including those that would otherwise not be used by the FlightSafety client.  The timing test was performed 100 times alternating between loading from the Shapefile CDB dataset, and the equivalent GeoPackage CDB dataset.
- The last row represents the relative performance of GeoPackage as compared to Shapefiles, with a number higher than 1.0 representing increased speed.

[width="90%",options="header"]
|===
|*Point Vectors*| Northwest Pacific	| Yemen	|Los Angeles
|*Feature Count* |	16,384	|5,552	|4,734
|*Shapefile Size* |	1.91 MB	|1.40 MB	|3.63 MB
|*GeoPackage Size* |	3.93 MB	|1.46 MB	|1.18 MB
|*Shapefile Read* |	55.8 ms	|64.4 ms	|17.4 ms
|*GeoPackage Read* |	82.3 ms	|36.78 ms	|39.9 ms
|*Relative GeoPackage +
Performance +
(>1.0 is faster)* |	0.678	|1.751	|0.437
|===

GeoPackage performance numbers were mixed for point data.  The GeoPackage performance seems linear with the number of features, but the Shapefile API tested was much faster on one case (Los Angeles) and much slower on another (Yemen).

Note:  _The Northwest Pacific dataset uses a minimal number of class-level attributes, resulting in a larger flattened GeoPackage size.  In contrast, the Los Angeles dataset uses mostly unique class-level attributes, which yields a larger overall Shapefile size, but smaller GeoPackage size because fewer class-level attributes needed to be duplicated._

[width="90%",options="header"]
|===
|*Line String Vectors*| Northwest Pacific	| Yemen	|Los Angeles
|*Feature Count* |	8,183	|2,457	|3,343
|*Shapefile Size* |	1.96 MB	|0.71 MB	|2.83 MB
|*GeoPackage Size* |	2.65 MB	|1.08 MB	|1.18 MB
|*Shapefile Read* |	62.2 ms	|26.3 ms	|17.4 ms
|*GeoPackage Read* |	49.9 ms	|19.0 ms	|23.1 ms
|* Relative GeoPackage +
Performance +
(>1.0 is faster)* |	1.246	|1.383	|1.225
|===

The use of GeoPackage increased performance across the board when linear data (22-38%) is processed and used.

[width="90%",options="header"]
|===
|*PolygonVectors*| Northwest Pacific	| Yemen	|Los Angeles
|*Feature Count* |	94	|198	|127
|*Shapefile Size* |	388 kB	|387 kB	|126 kB
|*GeoPackage Size* |	512 kB	|556 kB	|188 kB
|*Shapefile Read* |	9.3 ms	|10.0 ms	|7.3 ms
|*GeoPackage Read* |	6.3 ms	|6.4 ms	|4.9 ms
|*Relative GeoPackage +
Performance +
(>1.0 is faster)* |	1.476	|1.569	|1.502
|===

Larger performance increases for areal data (47% - 56%), at the cost of relatively larger storage size. However, the sample size (number of polygon features) is quite small.

==== GeoPackage Option 3 and 4 Testing

Please remember that in the FlightSafety presentation of results:

Option 1 = Experiment 2 +
Option 3 = Experiment 3 +
Option 4 = Experiment 4 +

===== Option 3 & 4 Conversion Statistics

In addition to the one-to-one Shapefile to GeoPackage encoding, we wished to also test the other GeoPackage encodings represented by options 3 and 4.  Conversions were performed to create these new CDB datasets using the modified python conversion scripts.  These were tested against the option 1 CDB datasets used in the previous tests.  The dataset statistics (file sizes and counts) are in the table below.  Conversion notes include:

- The parts of the file name (dataset code/component selectors/lod/row/column values) were initially stored as strings.  Converting these to integers led to about a 10% improvement over the initial string conversion.
- Index tables were created for the parts of the filename that did not comprise the table name.  This led to significant improvements that were up to 90% faster than without the index. 
-- Option 3 table names were of the form:  "D100_L04_S001_T001".  So indexes were created for the row and column values, assuming that a user might want to generate a SQL query on that table's row and column values.
-- Option 4 table names were of the form:  "D100_S001_T001".  So indexes were created for the lod, row and column values, assuming that a user might want to generate a SQL query on that table's lod and row and column values.

[width="90%",options="header"]
|===
|*Conversion Statistics*| Northwest Pacific	| Yemen	|Los Angeles
|*GeoPackage 1 File Count* |	25,083 files	|1,146 files	|2,615 files
|*GeoPackage 3/4 File Count* |	161 files	|22 files	|7 files
|*% Fewer Vector Files +
Options 3/4 vs Option 1* |	99.4 %	|98 %	|99.7 %
|===

As expected the number of files is much smaller using options 3 or 4.

[width="90%",options="header"]
|===
|*Conversion Statistics*| Northwest Pacific	| Yemen	|Los Angeles
|*Shapefile Storage Size* |	9,152 MB	|53.4 MB	|2,381 MB
|*GeoPackage 1 Storage Size* |	17,827 MB	|157.9 MB	|938 MB
|*GeoPackage 3 Storage Size* |	16,479 MB	|59.3 MB	|728 MB
|*GeoPackage 4 Storage Size* |	16,729 MB	|55.5 MB	|740 MB
|*Options 1 & 3 Relative Size +
(> 1 is larger)* |	0.92	|0.38	|0.78
|*Options 1 & 4 Relative Size +
(> 1 is larger)* |	0.94	|0.35	|0.79
|*Shapefile vs Option 3 Size +
(> 1 is larger)* |	1.80	|1.11	|0.31
|*Shapefile vs Option 4 Size +
(> 1 is larger)* |	1.83	|1.04	|0.31
|===

In all cases, the combined GeoPackage datasets required less storage than the option 1 GeoPackage files. This was true even though the combined datasets have index tables that the Option 1 GeoPackages do not have.  Note that in all cases even combining the GeoPackage files into a minimal set does not lead to a smaller vector dataset than Shapefiles.

===== Testing Procedure
The testing focus for comparing the different GeoPackage encoding options was on the latency of loading the GeoPackage file and using SQL queries to return the records converted from a single Shapefile.  This approach is similar to the Shapefiles vs GeoPackage testing done above, but the test was constructed slightly differently.

-	The GeoPackage was opened and an SQL query was performed to return the data that represented a single Shapefile's vector data.  In each query, the number of records in the table varied according to the type of conversion performed, but the amount of data and the values returned by the query were identical.
-	The SQL queries used for each GeoPackage option were variations on the following: 
--	Option 1:  SELECT * FROM 'D100_S001_T001_L04_U12_R0'
--	Option 3:  SELECT * FROM '100_GSFeature_L04_S001_T001' WHERE _UREF='12' AND _RREF='0'
--	Option 4:  SELECT * FROM '100_GSFeature_S001_T001' WHERE _LOD='4' AND _UREF='12' AND _RREF='0'
-	For timing purposes, each GeoPackage's open and query was run 100 times, alternating between each option test in succession.
-	The relative speed row shows the performance hit of doing an open on a larger GeoPackage with more tables and more records to search through.  For example, a 2.0 represents a test that took twice as long as the option 1 test.

[width="90%",options="header"]
|===
|*Point Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 Table Size (Count)* |	16,384	|5,552	|4,734
|*Option 1 Read GeoPackage* |	87.3 ms	|39.6 ms	|38.6 ms
|*Option 3 Table Size (Count)* |	3,375,935	| 7,766	| 493,936
|*Option 3 SQL Query* |	138.2 ms	|59.3 ms	|51.5 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.63	|0.67	|0.75
|*Option 4 Table Size (Count)*|	6,865,325	|43,122	|2,842,150
|*Option 4 SQL Query*|	173.9 ms	|44.9 ms	|79.7 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.50	|0.88	|0.48
|===

Option 3 GeoPackage file opens are sensitive to the number of tables in the GeoPackage, and tend to dominate the timing of cases with fewer features.  Option 4 has fewer tables and faster GeoPackage opens, but is more sensitive to the number of records in the table that need to be searched.

[width="90%",options="header"]
|===
|*Line String Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 Table Size (Count)* |	8,183	|2,457	|3,343
|*Option 1 Read GeoPackage* |	53.8 ms	|22.3 ms	|26.4 ms
|*Option 3 Table Size (Count)* |	16,454	|2,457	|80,697
|*Option 3 SQL Query*|	63.6 ms	|24.5 ms	|28.7 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.85	|0.91	|0.92
|*Option 4 Table Size (Count)* |	79,512	|2,457	|149,757
|*Option 4 SQL Query* |	52.4 ms	|23.0 ms	|29.7 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	1.03	|0.97	|0.89
|===

In both options 3 and 4, GeoPackage files perform slightly worse, but perform better than the point query because of fewer features returned.

[width="90%",options="header"]
|===
|*Polygon Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 Table Size (Count)* |	94	|198	|127
|*Option 1 Read GeoPackage* |	5.4 ms	|6.1 ms	|4.3 ms
|*Option 3 Table Size (Count)* |	207	|330	|1,238
|*Option 3 SQL Query* |	11.3 ms	|16.2 ms	|6.3 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.48	|0.37	|0.69
|*Option 4 Table Size (Count)* |	2,250	|1,531	|1,480
|*Option 4 SQL Query* |	6.1 ms	|7.4 ms	|5.0 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.88	|0.82	|0.86
|===

The overhead of opening GeoPackage files with lots of tables in the option 3 encoding is particularly prominent in the polygon queries.  The option 4 encoding is close to the single vector file per GeoPackage timing.

==== Further GeoPackage Option 3 & 4 Testing

===== Testing Procedure

The initial SQL query testing only performed a single query per GeoPackage open and close.  A more typical use case with options 3 and 4 would be to hold a GeoPackage file open for longer periods of time, and perform more queries per file access.  In this test, the same query was performed, but 100 queries were performed while the file was open.  There are limitations to the results of this test, as the same query was performed over and over.  It was likely that the parts of the file being accessed remained in memory the whole time, and this only measures the time to copy the data out of the GeoPackage file.  But it is a starting point toward understanding the performance of repeated queries in a large file.

The test results show the time per query, plus a 1/100 portion of the GeoPackage open and close time.  It also compares this time with Option 1's performance, where there is little gained by keeping the GeoPackage open.

===== Test Results

[width="90%",options="header"]
|===
|*Points - 100 Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 - 1 Query* |	87.3 ms	|46.0 ms	|38.6 ms
|*Option 3 - 100 Queries Average* |	64.6 ms	|25.9 ms	|26.4 ms
|*Percent Faster than Option 1* |	26%	|35%	|32%
|*Option 4 - 100 Queries Average* |	68.2 ms	|24.2 ms	|23.8 ms
|*Percent Faster than Option 1* |	22%	|39%	|38%
|===

Keeping the GeoPackage open between queries improves performance.  But note that not all cases are faster than the original Shapefile performance.

[width="90%",options="header"]
|===
|*Line Strings - 100 Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 - 1 Query* |	53.8 ms	|22.3 ms	|26.4 ms
|*Option 3 - 100 Queries Average* |	34.1 ms	|11.8 ms	|13.3 ms
|*Percent Faster than Option 1* |	37%	|47%	|49%
|*Option 4 - 100 Queries Average* |	34.9 ms	|11.1 ms	|13.3 ms
|*Percent Faster than Option 1* |	35%	|50%	|49%
|===

[width="90%",options="header"]
|===
|*Polygon - 100 Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 - 1 Query* |	5.4 ms	|6.1 ms	|4.3 ms
|*Option 3 - 100 Queries Average* |	1.1 ms	|1.6 ms	|0.78 ms
|*Percent Faster than Option 1* |	80%	|75%	|82%
|*Option 4 - 100 Queries Average* |	0.79 ms	|1.1 ms	|0.85 ms
|*Percent Faster than Option 1* |	85%	|82%	|80%
|===

This approach shows that there is some significant overhead in opening large GeoPackage files.  Keeping the GeoPackages open can mitigate some of the overhead.  We do not believe that a full client would see this level of performance, but there is a good possibility a client would see improved performance over option 1.

=== Hexagon US Federal Technology Experiment Report
==== Experiment Methodology - Dataset Conversion

*Experiment 1* - While the initial direction of the Interoperability Experiment involved utilizing the provided open source scripts to facilitate the conversion, there was an interest expressed from a Data Creator role in performing this operation with other software. CDB Studio features the capability to both ingest and generate CDB data stores so this was a natural fit for this application and Hexagon US Federal’s participation. The conversion process was developed to align closely with the provided workflow but was slightly altered in specific ways in line with how the application ingests data. Although an exhaustive analysis of the differences between the custom conversion logic and the provided scripts was not performed, where possible any differences are noted in this report. For Experiment 1, the Shapefile data was converted by adding the feature geometry and instance-level attributes into a GeoPackage using built-in capabilities in the LuciadLightpseed API. Class attributes were stored in a separate table using the pattern in Option 1b where the CNAM attribute for each feature is a foreign key for the class attributes table. The extended attributes were stored in a separate RTE table as in Option 1d and linked by a mapping table in accordance with the RTE spec.

*Experiment 2* – For this experiment the vector feature and attribute information for each tile was converted into a single GeoPackage dataset. The CDB directory structure was maintained with folders under each vector component for the levels of detail and U designation and individual GeoPackages for each R offset.

*Experiment 3* – In this experiment all datasets for a specific vector component were combined into a single GeoPackage. The resulting dataset differed slightly from the suggested approach in that the individual tile datasets were not combined into a feature layer in the GeoPackage and instead were kept in separate layers. This facilitated the current architecture of CDB Studio which was built to ingest the data in a tiled manner. It is entirely possible to adjust this pattern to utilize combined features and leverage spatial queries against this larger feature table which might produce an ingest performance increase, but this modification was beyond the scope of the Interoperability Experiment. Class attributes were consolidated for the component into a single table.

*Experiment 4* – For the final experiment all the component datasets were further combined to create a single GeoPackage per geocell. As in Experiment 3, the class attributes were combined into a single table per component.

==== Experiment Methodology - Visualization

The visualization methodology was consistent between all experiments and relied heavily on the existing logic of the CDB Studio application with minor modifications. Standard visualization metrics were recorded and did not differ greatly between the original and the experimental datasets which was expected as the LuciadLightspeed API was designed in the MVC paradigm and keeps the source data and display components independent. Data ingestion was shown to be the more pertinent metric affecting the overall performance of the visualization software. CDB data stores are ingested using a lazy loading strategy to alleviate memory concerns. The current implementation did not utilize the extended attributes so although these were added to the converted GeoPackages they had no impact on the data ingest for visualization. CDB Studio was also designed to match the CDB data stores’ tiled architecture and data loads are done as tiles are needed to populate the current display area and scale. Further modifications could be done to improve efficiency with the vector data stored in the combined GeoPackages as in experiments 3 and 4 and produce faster data ingestion but this was out of scope for this Interoperability Experiment.

Visualization metrics were gathered on the initial load of the CDB datasets which involves an animated pan/zoom to the dataset area and an initial display of the data in a view encompassing the entire dataset bounds and at a coarse detail level.

==== Metrics

.Yemen
[width="90%",options="header"]
|===
| | Shapefile | Experiment 2 | Experiment 3 | Experiment 4
| Time to Convert (s) | N/A | 475 | 392 | 505
| Size on Disk (MB) | 52.2 | 156 | 61.8 | 60.2
| File Count | 9000 | 1011 | 24 | 4
| Data Ingest (s) | 0.82 | 0.30 | 3.07 | 3.80
| Heap Memory Usage (MB) | 146 | 176 | 192 | 271
| Frames Per Second 4+^.^| 170-210
|===

.Downtown LA
[width="90%",options="header"]
|===
| | Shapefile | Experiment 2 | Experiment 3 | Experiment 4
| Time to Convert (s) | N/A | 1626 | 21436 | 37113
| Size on Disk (MB) | 2389.1 | 1075.2 | 840.0 | 839.5
| File Count | 15198 | 2533 | 7 | 1
| Data Ingest (s) | 0.26 | 0.69 | 134.3 | 124.3
| Heap Memory Usage (MB) | 210 | 144 | 4368 | 7061
| Frames Per Second 4+^.^| 170-240
|===

==== Notes on Metrics

*Time to Convert* – This is the total time to process the conversion of the dataset from the original version containing Shapefile data into GeoPackage version. Deletion of the original shapefile data was done as a secondary manual step and was not included in this metric.

*Size on Disk* – The size on disk was obtained by viewing the Windows Explorer properties window on the Tiles folder of the generated data. It is not inclusive of unmodified data components such as imagery and elevation. The size reported for the original Shapefile dataset was a measure of the replaced files.

*File Count* – Similar to the size on disk, this metric was gathered by viewing the Windows Explorer properties view on the Tiles folder of the generated data and does not include data components that were unaffected by the GeoPackage conversion process.

*Data Ingest* – Data ingest is the time taken to load all vector data needed the initial display of the CDB data. The visualization logic loads data on demand for the given tiles so this is a subset of the full dataset and could vary greatly depending on the areas and levels of detail being viewed. This metric also only reflects the modified vector datasets.

*Memory Usage* – Similar to the data ingest metric, the memory usage is a measure of the heap memory footprint of the ingested data repeated across the different sample datasets. The number reported was the delta of general system heap memory utilized before loading the CDB dataset and after. Note that this is not the final memory footprint of the datasets but instead can include temporary data structures used during the ingest process that could later be garbage collected. As such this metric is meant to show the typical memory overhead involved with loading and visualizing a CDB dataset and not the continuing persistent state.

*Frames Per Second (FPS)* – The frames per second values were gathered by utilizing a diagnostic overlay in the CDB Studio application during the data ingest. This value fluctuates during the tests and a visual inspection of the overlay data was used to determine the typical range of these fluctuations.

==== Notes and Observations

-	The relative times for the dataset conversion differed with the size of the datasets involved. Generating the E3 and E4 datasets for Yemen were roughly on par with or faster than E2’s one-to-one conversion, but the large LA dataset showed a vast increase in time for E3 and E4 so the value of the specific method is tied to the expected use case. The E2 dataset with the one-to-one Shapefile to GeoPackage conversion was still faster than the original dataset which indicates that this increase involves GeoPackage access and scalability with large datasets.

-	The size of the GeoPackage datasets were above that of the original Shapefile data. This may be due to overhead and additional metadata in the GeoPackage format as it was much more prevalent on E2’s one-to-one conversion.

-	Adaptations to the architecture of CDB Studio could further improve efficiency with GeoPackage. Enhancements in the LuciadLightspeeed API could also aid performance, such as built-in RTE handling to minimize JDBC connections.

-	Backwards compatibility was built in with minimal effort in the modifications by defining a hierarchy of where to look for vector data. This order was arbitrarily chosen for this Interoperability Experiment, but a suggestion would be to define this order as part of a revision to the CDB standard.

=== Guidance
A couple of performance comments (so far):

. Structure of the data matters.  Timing differences in SQL queries on integers rather than strings is enough to matter.
. As mentioned by others, opening a GeoPackage with lots of tables is slower than having a single table (option 3).
 .Doing a query to get features out of a very large table is MUCH slower (option 4).  I am getting 40x slowdowns for heavily forested areas where I am querying 4700 points out of a table with >2.8M points.
. The more columns a table has, the larger the slowdown (ie, a query in option 4 vs a query in option3 might take twice as long with 8 columns, but 4 times as long with 30 columns)
.. Depending on how much time we have left, testing option 1b might be worthwhile.  It should yield faster queries to not flatten class-level attributes into the feature table.
