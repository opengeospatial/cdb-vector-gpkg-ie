[[ResultsClause]]
== IE Experiment Results

This section of the Engineering Report provides details of the results of the experiments performed by each of the IE participants.

=== Aechelon Technology IE Report

==== Use case and experiment focus

The following is a description of a key use case in the Aechelon content processing workflow to use in an image generator.

* A CDB data store is used as the source for content to feed a publishing process into the Aechelon Image Generator (IG) runtime format. While reducing CDB storage requirements is a consideration, the primary concern in this workflow is with read access speed. Even so, the time taken by the 'feature scan' step of the publishing process, where the CDB source vector files are scanned to identify the features to import, is 2 orders of magnitude smaller than the rest of the pipeline. However, any improvements in speed and/or storage requirements have a positive impact on the efficiency of the Achelon publication workflow.
* Note: After the feature scan step in the publishing process, all references to all features are in various python data structures, which are then given to the first of multiple processing steps to begin the data transformations. For example, point features with models will have their OpenFlight models converted to an intermediate Aechelon-specific model format, while their instance geographical data are saved in an Aechelon-specific lookup table format.

The publishing software is in python, with invocations of C++ EXEs for performance-critical processing. The feature scan step is entirely in Python, version 3.5. Please note that for the image generation workflow, only metadata fields that affect the appearance of features are considered and remainder of the CDB content is ignored, such as tactical data, or the entire geopolitical dataset. The hydrography network dataset is also ignored since the RMTexture dataset is used to identify areas of water. 

For the purposes of this experiment, five feature types were considered and processed: Cultural, Lights, Powerlines, Railroads, and Trees. All of the performance information provided in the tables below are related to these five feature types.

The changes implemented on the publisher side to support GeoPackage were the minimal necessary to get functional parity with the Shapefile based implementation. In other words, no attempt was made to optimize the code to take advantage of the internals of the GeoPackage files using sqlite, and all data access went through the OGR module.

==== Aechelon Experiment Methodology

The following is a concise description of the methodolocy used for execution of the Aechelon committed tasks in this Interoperability Experiment.

* Following generation of the GeoPackage files for each option, changes were made as needed in the publishing scripts to be able to read and publish the data.
* Each feature type was tested by spot-checking in the image generator using a small reference database with representative data for each of the five feature types.
* The next step was to convert the following three of the four CDB data stores made available for the interoperability experiments. However, the entire publication end-to-end workflow for image generation was not performed due to the considerable time it would take for each run:
  ** Yemen (4 geocells), from Presagis.
  ** Downtown Los Angeles (1 geocell), from VATC.
  ** Greater Los Angeles (4 geocells), from Cognitics.
* To generate data for Experiment 2, option 1A:
  ** Ran Option1.py from the Cognitics conversion scripts in the master branch (https://github.com/Cognitics/GeoCDB/tree/master).
  ** Deleted existing .shp, .shx, sidecar .dbf, .dbt & .prj files (i.e. kept .dbf files holding class/extended data.)
* To generate data for Experiment 2, option 1C:
  ** Ran Option1.py from the Cognitics conversion scripts in the Presagis branch (https://github.com/Cognitics/GeoCDB/tree/Presagis).
  ** Deleted the existing .shp, .shx, .dbf, .dbt & .prj files.
* To generate data for Experiment 2, option 1D:
  ** Ran Option1d.py from the Cognitics conversion scripts in the master branch (after update of March 17, 2019, and some local edits to protect against 'None' during conversion of the LA databases.)
  ** Deleted the existing .shp, .shx, .dbf, .dbt & .prj files.
* To generate data for Experiment 3:
  ** Ran Option3.py from the Cognitics conversion scripts in the master branch (after update of March 17, 2019, and some local edits to uncomment writing the class metadata to the instance tables and to protect against 'None' during conversion of the LA databases.)
  ** Deleted the 100_GSFeature, 101_GTFeature, 202_RailroadNetwork and 203_PowerlineNetwork folders from each geocell.
* To generate data for Experiment 4:
  ** Ran Option4.py from the Cognitics conversion scripts in the master repository (after update of March 17, 2019, and some local edits to uncomment writing the class metadata to the instance tables and to protect against 'None' during conversion of the LA databases.)
  ** Deleted the 100_GSFeature, 101_GTFeature, 202_RailroadNetwork and 203_PowerlineNetwork folders from each geocell.
* Then, for each option, disabled the publishing process beyond the 'feature scan' step and captured the following metrics for the three CDB data stores.

==== Metrics

The following three tables provide basic performance metrics for the three CDB data stores processed in this experiment. Providing performance metrics is one of the tasks identified in Experiment 1 of the GeoPackage in CDB IE.

In the tables below, "Baseline" refers to metrics based on the source ShapeFiles. Option 1A, Option 1C, and Option 1D refer to the sub-options for Experiment 2 (one to one transformation of Shapefiles into GeoPackages).

.Yemen (4 geocells)
[width="90%",options="header"]
|===
|           |           |          |Baseline  |Option 1A |Option 1C |Option 1D |Exper. 3 |Exper. 4     
|Dataset(s) |Feat count |PVF count |     time |     time |     time |     time |    time |    time
|tree       |     64091 |     440  |        8 |        7 |        7 |       16 |       6 |       2
|light      |        60 |      13  |       <1 |       <1 |       <1 |       <1 |       1 |       1
|cultural   |     16502 |     409  |       12 |        9 |        5 |        7 |       5 |       4
|powerline  |       975 |      20  |       <1 |       <1 |       <1 |       <1 |       1 |       1
|railroad   |         0 |       0  |        0 |        0 |        0 |        0 |       0 |       0
|total time |  | |                         21 |       17 |       13 |       24 |      14 |       8
|file count |  | |                       8224 |     2056 |     1023 |     1023 |      10 |      10           
|size (MB)  |  | |                        34.2|     152.5|     161.9|     165.9|     57.6|     38.1 
|===

                                                                                               
.Downtown Los Angeles (1 geocell)
[width="90%",options="header"]
|===
|           |           |          |Baseline  |Option 1A |Option 1C |Option 1D |Exper. 3 |Exper. 4     

|Dataset(s) |Feat count |PVF count |     time |     time |     time |     time |    time |    time
|tree       |        2  |        1 |       <1 |       <1 |       <1 |       <1 |      <1 |      <1
|light      |        0  |        0 |        0 |        0 |        0 |        0 |       0 |       0
|cultural   |  1730622  |     1948 |     9:01 |     7:13 |     3:06 |     3:34 |    3:26 |    3:41
|powerline  |     1208  |       56 |        2 |        1 |        1 |        1 |      <1 |       1
|railroad   |     1386  |        4 |        1 |       <1 |       <1 |       <1 |      <1 |       1
|total time |   ||                      9:04 |     7:15 |     3:08 |     3:36 |    3:27 |    3:44          
|file count |   ||                     12540 |     4180 |     2090 |     2090 |       4 |       4       
|size (MB)  |   ||                     2185.7|    2309.2|     958.5|    1021.5|    791.5|    798.0
|===

.Greater Los Angeles (4 geocells)
[width="90%",options="header"]
|===
|           |           |          |Baseline  |Option 1A |Option 1C |Option 1D |Exper. 3 |Exper. 4     

|Dataset(s) |Feat count |PVF count |     time |     time |     time |     time |    time |    time
|tree       |        5  |        2 |       <1 |        1 |       <1 |       <1 |       1 |      <1
|light      |        0  |        0 |        0 |        0 |        0 |        0 |       1 |      <1
|cultural   |  3138841  |     6013 |    15:02 |    12:02 |     6:14 |     7:25 |    6:57 |    7:17
|powerline  |     3932  |      160 |        1 |        1 |        1 |        1 |       1 |       1
|railroad   |     9367  |       87 |        1 |        1 |        1 |        1 |       1 |      <1
|total time |      ||                   15:04 |    12:05 |     6:16 |     7:27 |    7:01 |    7:19
|file count |      ||                   38961 |    12986 |     6493 |     6493 |      14 |      14         
|size (MB)  |      ||                   3738.2|    4275.9|    1958.6|    2067.0|   1335.7|   1339.3
|===

==== Legend

* Feat count: feature count of valid features found of the given type
* PVF count: primary vector file count, after validation, for the given type (i.e. only counting .shp files for Experiment 1 or .gpkg files for Experiment 2.)
* Time: in minute:second notation when over 1 minute, else in seconds
* The cultural feature data set is from both 100_GSFeatures (S001_T001 & S002_T001) and 101_GTFeatures (S001_T001)
* File count: total number of files from 100_GSFeatures, 101_GTFeatures, 202_RailroadNetwork & 203_PowerLineNetwork
* Size: storage, in MB, used by all the files from 100_GSFeatures, 101_GTFeatures, 202_RailroadNetwork & 203_PowerLineNetwork

==== Notes and observations
* All source CDB files were on a local RAID drive so network traffic did not contribute to the timings.
* In the Greater Los Angeles database, there somehow were more features of some types coming from geopackage files compared to shape files (3140180 instead of 3138841 cultural features, and 4012 instead of 3932 powerline features), but there were also over 1000 warnings from OGR during conversion and while reading of the type "Warning 1: Unable to parse srs_id '100000' well-known text ''." After the 1000th such warning, also got "More than 1000 errors or warnings have been reported. No more will be reported from now."  Perhaps the conversion from .shp to .gpkg with ogr2ogr.exe generated these excess invalid files. These warnings appeared in the Downtown LA database as well, but the feature counts matched after conversion. Checking any further downstream for discrepencies in the processing pipeline was not performed.
* For the powerline network dataset, statistics include both the tower point features and the wire lineal features.
* There's a slight increase in the file size in the Los Angeles databases when comparing the results of Experiment 3 and Experiment 4. However, there is a significant decrease in the size of the Yemen database. From a quick inspection of the data, this seems to correlate with the fact that almost all the cultural features in Los Angeles come from 100_GSFeatures which require unique records per instance, whereas for Yemen the majority of cultural features come from 101_GTFeatures.
* Experiment 3 has slightly better timings for large-count datasets than Experiment 4 in our use case since we scan each LOD in order, so having LODs in separate layers in the option 3 GeoPackage performs better.

==== Compusult metrics from Experiment 2
Approach: One GeoPackage per LOD per dataset

CDB: CDBYemen_4.0.0

Available Datasets:

- 101_GTFeature
- 100_GSFeature
- 401_Navigation
- 201_RoadNetwork

Number of ShapeFiles processed: 358
Number of GeoPackages created: 18
Total byte size of ShapeFiles (bytes): 3,569,324
Total byte size of GeoPackages (bytes): 41,715,712
Elapsed time (seconds): 173

=== FlightSafety Experiment Results

==== FlightSafety International's Use Case for CDB
FlightSafety has developed both a CDB generation tool and a CDB Publisher client.  The performance requirements of the CDB Publisher are much greater than CDB generation, so this report will focus on loading and consuming a CDB dataset. The CDB Publisher uses a CDB data store as the source data for building the synthetic environment for FlightSafety's VITAL 1100 image generator system.  These systems are used for pilot training on a variety of flight simulator systems. The Publisher does not do any preprocessing of the CDB dataset; all CDB data that it consumes is discovered and loaded during the publishing.  This approach was chosen due to the world-wide scope of CDB and unknown quantity of content.  The CDB specification's structure makes it easy to find the file(s) containing the data needed for the synthetic environment creation. Based on the flight training system requirements, an appropriate level of detail of vector and model data is discovered and loaded.  The Publisher adapts to the available levels of detail of vector data, and the flight characteristics of the training device. The publishing system is primarily in C++, and the testing was all performed with C++ libraries and code.  The Shapefile API that is tested is a custom FlightSafety library, optimized for faster performance.

==== FlightSafety Experiment Focus
The experiments were focused on just-in-time uses of CDB, similar to how a FlightSafety visual system would use the data. Statistics were collected on the original CDB dataset, and the converted GeoPackage CDB datasets.  These were used to infer the cost of database configuration management and transmission/deployment to a training device. Testing was done on both currently encoded CDB shapefiles, and on converted GeoPackage encoded files (covering options 1, 3, and 4). Tests focused on the latency of loading files, processing data, and closing files. Tests were done on different conversion options and settings to come up with optimal recommendations

==== FlightSafety Experiment Methodology

This is the methodology used to evaluate, convert and test the CDB datasets using GeoPackage vector encoding.

===== Data Acquisition: 
Three CDB datasets were downloaded (from Cognitics, Presagis, and VATC) and loaded on a system. The datasets were then split into two CDBs, one of which contains all vector data and the other contains everything else.  They were linked together using CDB's versioning mechanism, so that the FlightSafety publisher sees the data as a single dataset. Further:
- Any official or unofficial extension to the CDB was removed for testing purposes.
- Any 0 size vector file was deleted from the CDB with vector data.  These were 0 size shp and shx files for datasets that should only be dbf, and cases of 0 size dbt files when they weren't needed alongside their dbf parent file.

===== Data Evaluation: 
All three CDB datasets were flown using FlightSafety's VITAL 1100 image generator and CDB publisher. During the fly-through, any data artifacts were noted and recorded.

===== Data Conversion: 
The python conversion scripts developed by Cognitics, Inc. were downloaded from GitHub. The scripts were modified to properly flatten class-level attributes into the feature table, and to properly handle DBase floating point and logical field types.  Index tables were also added to aid SQL queries designed to get back data for a specific CDB vector file. Script changes were published to a public GitHub under a FlightSafety account (link). When the scripts were run, they created a new output directory for the CDB vector data.  The Metadata folder was copied from the original vector CDB version, which then links this GeoPackage version to the rest of the CDB data. The three main conversion scripts used implemented GeoPackage encoding options 1, 3, and 4.

===== GeoPackage Testing: 
The initial data collection centered on the number of vector files and how much disk space was consumed.  All full CDB storage devices used a 4kB block size and recorded sizes include the "dead" space due to the minimum block size. The initial tests were testing shapefiles vs. option 1.  All vector files were located, and timed on the file open and accessing the data within the file.  Total processing time was recorded and compared between the two encodings.  This test accessed the geometry and all the attributes, whether they would have been used by the FlightSafety CDB publisher or not.

The next set of tests involved working with worst case examples and comparing the same file open and access time as before, but for single files.  This highlights performance on the largest vector files.  The average performance times are reported here. 

Further testing was performed to see what the trade offs were between options 1, 3, and 4.  These included loading identical vectors (from a single original shapefile) from each of three GeoPackage files converted in different ways: 

- GeoPackage Option 1 (Experiment 2 in the IE Activity Plan) was a straight conversion of the shapefile.  The GeoPackage contains a single data table with flattened class-level attributes, with the same number of records as the original shapefile
- GeoPackage Option 3 (Experiment 3 in the IE Activity Plan) was a conversion of each CDB dataset's features into a table for each level of detail (LOD) and component selector set, placed into a single GeoPackage (1 per dataset).  It also contained the most tables, and typically had more feature records than option 1 but fewer than option 4.
- GeoPackage Option 4 (Experiment $ in the IE Activity Plan) was a conversion of each CDB dataset's features into a table for each component selector set, placed into a single GeoPackage (1 per dataset).  This method placed all levels of detail into the same table, resulting in a handful of tables, but possibly millions of features per table.

Note: _The results for Experiment 1 (Conversion) are provided in the discussions of Option 1, 3, and 4 (IE Experiments 2, 3, and 4)._

==== FlightSafety Metrics

===== Original Dataset Statistics
Basic statistics were collected on the original CDB datasets used in the Interoperability Experiment.  The CDB storage size and file counts do not include any 0-sized files (they weren't required by the CDB specification) and do not include non-standard extension data.  The last two rows represent the proportion of vector data in the CDB, by the percentage of files and storage used.  The vector datasets used are:
- 100_GSFeature
- 101_GTFeature
- 102_GeoPolitical
- 201_RoadNetwork
- 202_RailroadNetwork
- 203_PowerlineNetwork
- 204_HydrographyNetwork
- 401_Navigation

.Table of Dataset Statistics
[width="90%",options="header"]
|===
|           | Northwest Pacific|  Yemen|Los Angeles
|*Provider*	  |Cognitics	 |Presagis	|VATC
|*CDB Geocell Tiles*|	27|	4|	1
|*CDB Storage Size*|	214 GB	|17.4 GB	|59.6 GB
|*CDB File Count*|	427,536 files	|112,837 files	|62,895 files
|*Vector Storage Size*|	9,152 MB	|53.4 MB	|2,381 MB
|*Vector File Count*|	109,490 files	|4714 files	|13,075 files
|% of CDB storage as vectors	|4.18 %	|0.30 %	|3.90 %
|% of CDB files as vectors	|25.6 %	|4.18 %	|20.8 %
|===

The main takeaway from this table is that vector data does not consume a large amount of storage space, but accounts for a prodigious number of files within a typical CDB.  The main driver of file counts are that Shapefiles are a multi-file format, where three (or four with the .prj projection file) files represent a single Shapefile.  In addition to the multifile format, CDB uses extra class-level and extended-level attributes encoded as extra DBF files.  So anywhere from 3 to 8 files are used to represent a single logical vector file.

===== Specific Vector File Test Data
Some of the testing below involved loading specific point/linear/areal vectors that represent a single Shapefile.  For these tests, examples were found that represent "worst-case" examples of large vector files.  These larger files would take more time to load, and most occurred within higher LODs that would lead to larger tables in options 3 and 4.  The following table records the specific shapefile data for individual tests.

[width="90%",options="header"]
|===
|  | Northwest Pacific	| Yemen	|Los Angeles
|*Point Vector* |	N46W124_D101_S002_T001_L04_U15_R12	| N12E045_D100_S001_T001_L04_U12_R0	| N34W119_D100_S001_T001_L05_U8_R20
|*Linear Vector* |	N48W123_D201_S002_T003_L01_U0_R0	| N12E045_D201_S002_T003_L00_U0_R0	| N34W119_D201_S002_T003_L04_U1_R15
|*Areal Vector* |	N47W120_D204_S002_T005_L02_U0_R2	| N12E044_D100_S002_T005_L02_U3_R3	| N34W119_D204_S002_T005_L03_U4_R7
|===

==== Shapefile vs. GeoPackage Option 1 (Experiment 2) Testing

===== Option 1 Conversion Statistics
Before the first set of tests, the CDB datasets were converted one-to-one from shapefiles to GeoPackage, using the option 1 conversion.  Dataset statistics were then collected on the new datasets and compared with the original datasets.

[width="90%",options="header"]
|===
|  | Northwest Pacific	| Yemen	|Los Angeles
|*Shapefile Vector Storage Size* |	9,152 MB	|53.4 MB	|2,381 MB
|*Shapefile Vector File Count* |	109,490 files	|4714 files	|13,075 files
|*GeoPackage 1 Storage Size* |	17,827 MB	|157.9 MB	|938 MB
|*GeoPackage 1 File Count* |	25,083 files	|1,146 files	|2,615 files
|*Relative Size (>1 is larger)* |	1.95	|2.96	|0.39
|*% Fewer Vector Files* |	77 %	|76 %	|80 %
|===

File counts for the GeoPackage CDB were between a 4:1 and 5:1 reduction in vector files.  The size changes varied dramatically, likely due to how efficient the attributes were packed into the original Shapefile's instance and class-level DBF files.  In general, an increase in CDB size is expected using option 1.

===== Option 1 Testing Focus
The testing focused on the latency of loading and processing the vector data files, and traversing all the geometry features and attributes.  This approach was used to simulate a flight simulation client's use of CDB.

===== Test Procedure 1
The first test was to traverse the entire CDB dataset, find all the vector files and collect the time it took to open, process, and close each vector file.  For each dataset, every vector file was located by walking the directory structure, and then the file loading and processing was timed.  This test was run 30 times on the smaller CDB datasets (Yemen and Los Angeles) and 10 times on the larger Northwest Pacific dataset.  The sum of the file load and process steps are recorded below (while ignoring the file search times).

[width="90%",options="header"]
|===
|*All Vector Files*| Northwest Pacific	| Yemen	|Los Angeles
|*Shapefile Timing* |	835 sec	|10.2 sec	|27.5 sec
|*GeoPackage Timing* |	478 sec	|4.2 sec	|25.7 sec
|*GeoPackage Speed Comparison* |	42% faster	|58% faster	|6.7% faster
|*Average Shapefile Storage Size* |	374 kB	|48 kB	|923 kB
|===

This table shows, on average, that using GeoPackages are faster than using Shapefiles.  These results imply that GeoPackage has a better advantage with smaller files. For example, GeoPackage performed best on Yemen with its relatively small shapefile/vector files.  However, there is less of an advantage with larger vector files. Therefore, further testing using larger files is recommended.

===== Test Procedure 2
The next set of tests focused on some of the largest individual vector files. This test was performed to evaluate some of the worst case examples.  The exact file names are mentioned above in the Specific Vector File Test Data section.  These test datasets were much larger than the average vector file and cover the three basic geometry types: Points, Line Strings and Polygons.  This allowed testing of files that have many attributes compared to coordinates (points), and testing of files with many coordinates compared to the number of attributes (polygons).

- The file size for Shapefiles includes both the instance-level files (.shp, .shx, .dbf) and the class-level attributes (.dbf), but no extended attributes or projection information.  The GeoPackage file size was the single .gpkg file.
- The timing numbers include opening the file and traversing the geometry and every attribute in each record, including those that would otherwise not be used by the FlightSafety client.  The timing test was performed 100 times alternating between loading from the shapefile CDB dataset, and the equivalent GeoPackage CDB dataset.
- The last row represents the relative performance of GeoPackage as compared to Shapefiles, with a number higher than 1.0 representing increased speed.

[width="90%",options="header"]
|===
|*Point Vectors*| Northwest Pacific	| Yemen	|Los Angeles
|*Feature Count* |	16,384	|5,552	|4,734
|*Shapefile Size* |	1.91 MB	|1.40 MB	|3.63 MB
|*GeoPackage Size* |	3.93 MB	|1.46 MB	|1.18 MB
|*Shapefile Read* |	55.8 ms	|64.4 ms	|17.4 ms
|*GeoPackage Read* |	82.3 ms	|36.78 ms	|39.9 ms
|*Relative GeoPackage +
Performance +
(>1.0 is faster)* |	0.678	|1.751	|0.437
|===

GeoPackage performance numbers were mixed for point data.  The GeoPackage performance seems linear with the number of features, but the Shapefile API tested was much faster on one case (Los Angeles) and much slower on another (Yemen).

Note:  _The Northwest Pacific dataset uses a minimal number of class-level attributes, resulting in a larger flattened GeoPackage size.  In contrast, the Los Angeles dataset uses mostly unique class-level attributes, which yields a larger overall Shapefile size, but smaller GeoPackage size because fewer class-level attributes needed to be duplicated._

[width="90%",options="header"]
|===
|*Line String Vectors*| Northwest Pacific	| Yemen	|Los Angeles
|*Feature Count* |	8,183	|2,457	|3,343
|*Shapefile Size* |	1.96 MB	|0.71 MB	|2.83 MB
|*GeoPackage Size* |	2.65 MB	|1.08 MB	|1.18 MB
|*Shapefile Read* |	62.2 ms	|26.3 ms	|17.4 ms
|*GeoPackage Read* |	49.9 ms	|19.0 ms	|23.1 ms
|* Relative GeoPackage +
Performance +
(>1.0 is faster)* |	1.246	|1.383	|1.225
|===

The use of GeoPackage increased performance across the board when linear data (22-38%) is processed and used.

[width="90%",options="header"]
|===
|*PolygonVectors*| Northwest Pacific	| Yemen	|Los Angeles
|*Feature Count* |	94	|198	|127
|*Shapefile Size* |	388 kB	|387 kB	|126 kB
|*GeoPackage Size* |	512 kB	|556 kB	|188 kB
|*Shapefile Read* |	9.3 ms	|10.0 ms	|7.3 ms
|*GeoPackage Read* |	6.3 ms	|6.4 ms	|4.9 ms
|*Relative GeoPackage +
Performance +
(>1.0 is faster)* |	1.476	|1.569	|1.502
|===

Larger performance increases for areal data (47% - 56%), at the cost of relatively larger storage size. However, the sample size (number of polygon features) is quite small.

==== GeoPackage Option 3 and 4 Testing

Please remember that in the FlightSafety presentation of results:

Option 1 = Experiment 2 +
Option 3 = Experiment 3 +
Option 4 = Experiment 4 +

===== Option 3 & 4 Conversion Statistics

In addition to the one-to-one shapefile to GeoPackage encoding, we wished to also test the other GeoPackage encodings represented by options 3 and 4.  Conversions were performed to create these new CDB datasets using the modified python conversion scripts.  These were tested against the option 1 CDB datasets used in the previous tests.  The dataset statistics (file sizes and counts) are in the table below.  Conversion notes include:

- The parts of the file name (dataset code/component selectors/lod/row/column values) were initially stored as strings.  Converting these to integers led to about a 10% improvement over the initial string conversion.
- Index tables were created for the parts of the filename that did not comprise the table name.  This led to significant improvements that were up to 90% faster than without the index. 
-- Option 3 table names were of the form:  "D100_L04_S001_T001".  So indexes were created for the row and column values, assuming that a user might want to generate a SQL query on that table's row and column values.
-- Option 4 table names were of the form:  "D100_S001_T001".  So indexes were created for the lod, row and column values, assuming that a user might want to generate a SQL query on that table's lod and row and column values.

[width="90%",options="header"]
|===
|*Conversion Statistics*| Northwest Pacific	| Yemen	|Los Angeles
|*GeoPackage 1 File Count* |	25,083 files	|1,146 files	|2,615 files
|*GeoPackage 3/4 File Count* |	161 files	|22 files	|7 files
|*% Fewer Vector Files +
Options 3/4 vs Option 1* |	99.4 %	|98 %	|99.7 %
|===

As expected the number of files is much smaller using options 3 or 4.

[width="90%",options="header"]
|===
|*Conversion Statistics*| Northwest Pacific	| Yemen	|Los Angeles
|*Shapefile Storage Size* |	9,152 MB	|53.4 MB	|2,381 MB
|*GeoPackage 1 Storage Size* |	17,827 MB	|157.9 MB	|938 MB
|*GeoPackage 3 Storage Size* |	16,479 MB	|59.3 MB	|728 MB
|*GeoPackage 4 Storage Size* |	16,729 MB	|55.5 MB	|740 MB
|*Options 1 & 3 Relative Size +
(> 1 is larger)* |	0.92	|0.38	|0.78
|*Options 1 & 4 Relative Size +
(> 1 is larger)* |	0.94	|0.35	|0.79
|*Shapefile vs Option 3 Size +
(> 1 is larger)* |	1.80	|1.11	|0.31
|*Shapefile vs Option 4 Size +
(> 1 is larger)* |	1.83	|1.04	|0.31
|===

In all cases, the combined GeoPackage datasets required less storage than the option 1 GeoPackage files. This was true even though the combined datasets have index tables that the Option 1 GeoPackages do not have.  Note that in all cases even combining the GeoPackage files into a minimal set does not lead to a smaller vector dataset than Shapefiles.

===== Testing Procedure
The testing focus for comparing the different GeoPackage encoding options was on the latency of loading the GeoPackage file and using SQL queries to return the records converted from a single Shapefile.  This approach is similar to the Shapefiles vs GeoPackage testing done above, but the test was constructed slightly differently.

-	The GeoPackage was opened and an SQL query was performed to return the data that represented a single shapefile's vector data.  In each query, the number of records in the table varied according to the type of conversion performed, but the amount of data and the values returned by the query were identical.
-	The SQL queries used for each GeoPackage option were variations on the following: 
--	Option 1:  SELECT * FROM 'D100_S001_T001_L04_U12_R0'
--	Option 3:  SELECT * FROM '100_GSFeature_L04_S001_T001' WHERE _UREF='12' AND _RREF='0'
--	Option 4:  SELECT * FROM '100_GSFeature_S001_T001' WHERE _LOD='4' AND _UREF='12' AND _RREF='0'
-	For timing purposes, each GeoPackage's open and query was run 100 times, alternating between each option test in succession.
-	The relative speed row shows the performance hit of doing an open on a larger GeoPackage with more tables and more records to search through.  For example, a 2.0 represents a test that took twice as long as the option 1 test.

[width="90%",options="header"]
|===
|*Point Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 Table Size (Count)* |	16,384	|5,552	|4,734
|*Option 1 Read GeoPackage* |	87.3 ms	|39.6 ms	|38.6 ms
|*Option 3 Table Size (Count)* |	3,375,935	| 7,766	| 493,936
|*Option 3 SQL Query* |	138.2 ms	|59.3 ms	|51.5 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.63	|0.67	|0.75
|*Option 4 Table Size (Count)*|	6,865,325	|43,122	|2,842,150
|*Option 4 SQL Query*|	173.9 ms	|44.9 ms	|79.7 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.50	|0.88	|0.48
|===

Option 3 GeoPackage file opens are sensitive to the number of tables in the GeoPackage, and tend to dominate the timing of cases with fewer features.  Option 4 has fewer tables and faster GeoPackage opens, but is more sensitive to the number of records in the table that need to be searched.

[width="90%",options="header"]
|===
|*Line String Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 Table Size (Count)* |	8,183	|2,457	|3,343
|*Option 1 Read GeoPackage* |	53.8 ms	|22.3 ms	|26.4 ms
|*Option 3 Table Size (Count)* |	16,454	|2,457	|80,697
|*Option 3 SQL Query*|	63.6 ms	|24.5 ms	|28.7 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.85	|0.91	|0.92
|*Option 4 Table Size (Count)* |	79,512	|2,457	|149,757
|*Option 4 SQL Query* |	52.4 ms	|23.0 ms	|29.7 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	1.03	|0.97	|0.89
|===

In both options 3 and 4, GeoPackage files perform slightly worse, but perform better than the point query because of fewer features returned.

[width="90%",options="header"]
|===
|*Polygon Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 Table Size (Count)* |	94	|198	|127
|*Option 1 Read GeoPackage* |	5.4 ms	|6.1 ms	|4.3 ms
|*Option 3 Table Size (Count)* |	207	|330	|1,238
|*Option 3 SQL Query* |	11.3 ms	|16.2 ms	|6.3 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.48	|0.37	|0.69
|*Option 4 Table Size (Count)* |	2,250	|1,531	|1,480
|*Option 4 SQL Query* |	6.1 ms	|7.4 ms	|5.0 ms
|*Speed Relative to Option 1 +
(>1 is faster)* |	0.88	|0.82	|0.86
|===

The overhead of opening GeoPackage files with lots of tables in the option 3 encoding is particularly prominent in the polygon queries.  The option 4 encoding is close to the single vector file per GeoPackage timing.

==== Further GeoPackage Option 3 & 4 Testing

===== Testing Procedure

The initial SQL query testing only performed a single query per GeoPackage open and close.  A more typical use case with options 3 and 4 would be to hold a GeoPackage file open for longer periods of time, and perform more queries per file access.  In this test, the same query was performed, but 100 queries were performed while the file was open.  There are limitations to the results of this test, as the same query was performed over and over.  It was likely that the parts of the file being accessed remained in memory the whole time, and this only measures the time to copy the data out of the GeoPackage file.  But it is a starting point toward understanding the performance of repeated queries in a large file.

The test results show the time per query, plus a 1/100 portion of the GeoPackage open and close time.  It also compares this time with Option 1's performance, where there is little gained by keeping the GeoPackage open.

===== Test Results

[width="90%",options="header"]
|===
|*Points - 100 Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 - 1 Query* |	87.3 ms	|46.0 ms	|38.6 ms
|*Option 3 - 100 Queries Average* |	64.6 ms	|25.9 ms	|26.4 ms
|*Percent Faster than Option 1* |	26%	|35%	|32%
|*Option 4 - 100 Queries Average* |	68.2 ms	|24.2 ms	|23.8 ms
|*Percent Faster than Option 1* |	22%	|39%	|38%
|===

Keeping the GeoPackage open between queries improves performance.  But note that not all cases are faster than the original Shapefile performance.

[width="90%",options="header"]
|===
|*Line Strings - 100 Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 - 1 Query* |	53.8 ms	|22.3 ms	|26.4 ms
|*Option 3 - 100 Queries Average* |	34.1 ms	|11.8 ms	|13.3 ms
|*Percent Faster than Option 1* |	37%	|47%	|49%
|*Option 4 - 100 Queries Average* |	34.9 ms	|11.1 ms	|13.3 ms
|*Percent Faster than Option 1* |	35%	|50%	|49%
|===

[width="90%",options="header"]
|===
|*Polygon - 100 Queries*| Northwest Pacific	| Yemen	|Los Angeles
|*Option 1 - 1 Query* |	5.4 ms	|6.1 ms	|4.3 ms
|*Option 3 - 100 Queries Average* |	1.1 ms	|1.6 ms	|0.78 ms
|*Percent Faster than Option 1* |	80%	|75%	|82%
|*Option 4 - 100 Queries Average* |	0.79 ms	|1.1 ms	|0.85 ms
|*Percent Faster than Option 1* |	85%	|82%	|80%
|===

This approach shows that there is some significant overhead in opening large GeoPackage files.  Keeping them open can mitigate some of the overhead.  We do not believe that a full client would see this level of performance, but there is a good possibility a client would see improved performance over option 1.


== Guidance
A couple of performance comments (so far):

. Structure of the data matters.  Timing differences in SQL queries on integers rather than strings is enough to matter.
. As mentioned by others, opening a GeoPackage with lots of tables is slower than having a single table (option 3).
 .Doing a query to get features out of a very large table is MUCH slower (option 4).  I am getting 40x slowdowns for heavily forested areas where I am querying 4700 points out of a table with >2.8M points.
. The more columns a table has, the larger the slowdown (ie, a query in option 4 vs a query in option3 might take twice as long with 8 columns, but 4 times as long with 30 columns)
.. Depending on how much time we have left, testing option 1b might be worthwhile.  It should yield faster queries to not flatten class-level attributes into the feature table.

===Holder of info for inclusion

=== Who is doing what

==== Hexagon/Luciad

For our involvement as a participant in the current GeoPackage conversion IE we are at this point planning to complete all listed experiments (1 - 4).

Additionally, we are currently modifying our existing CDB client software to produce the converted vector data in the GeoPackage format and are implementing Option 1d of the conversion strategies.

We will be focused primarily on the client visualization performance for our contribution to the ER. As discussed on last week's call we will also provide some file system metrics after the data conversion. If time permits we will perform the conversions and experiments for all three datasets.

=== Metrics captured

==== FSI
For the client side, FSI documented the following metrics:

- Time to open a GeoPackage (plus SQLite overhead) vs reading/parsing multiple smaller files for shapefile (more I/O operations)
- Time to find/get a layer
- Time to close and dispose of a GeoPackage vs shapefile
- Time to query getting a set of features by SQL query of dataset/lod/row/column vs an rtree SQL search
